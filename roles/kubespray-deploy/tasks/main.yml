---
# ================================================================
# KUBESPRAY DEPLOY — runs on master1 only
#
# Steps:
#   1. Authorize SSH public key on ALL nodes (so master1 can reach them)
#   2. Push SSH private key to master1
#   3. Scan host keys on master1
#   4. Install system dependencies on master1
#   5. Clone Kubespray
#   6. pip install requirements into venv
#   7. Write inventory, addons.yml, k8s-cluster.yml
#   8. Run cluster.yml (backgrounded via nohup, polled every 30s)
#   9. Set up kubectl
# ================================================================

# ── Step 1: Authorize public key on ALL nodes ─────────────────────
# master1 SSHes to all other nodes during cluster.yml — every node
# must trust the cluster key, not just master1
- name: Authorize cluster SSH public key on all nodes
  authorized_key:
    user: "{{ ssh_user }}"
    key: "{{ lookup('file', ssh_public_key) }}"
    state: present
  delegate_to: "{{ item }}"
  loop: "{{ groups['all'] }}"
  loop_control:
    label: "{{ item }}"

# ── Step 2: Push SSH key to master1 ──────────────────────────────
# master1 needs the private key to initiate SSH connections to other nodes
- name: Ensure .ssh directory exists on master1
  file:
    path: "/home/{{ ssh_user }}/.ssh"
    state: directory
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0700'

- name: Push cluster SSH private key to master1
  copy:
    src: "{{ ssh_private_key }}"
    dest: "/home/{{ ssh_user }}/.ssh/id_rsa"
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0600'

- name: Push cluster SSH public key to master1
  copy:
    src: "{{ ssh_public_key }}"
    dest: "/home/{{ ssh_user }}/.ssh/id_rsa.pub"
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0644'

# ── Step 3: Scan host keys on master1 ────────────────────────────
# Prevents "host key verification failed" when master1 SSHes to other nodes
- name: Scan and register all node host keys on master1
  shell: |
    ssh-keyscan -t rsa {{ hostvars[item]['ip'] }} \
      >> /home/{{ ssh_user }}/.ssh/known_hosts 2>/dev/null
    sort -u /home/{{ ssh_user }}/.ssh/known_hosts \
      -o /home/{{ ssh_user }}/.ssh/known_hosts
  args:
    executable: /bin/bash
  loop: "{{ groups['all'] }}"
  loop_control:
    label: "{{ item }}"
  changed_when: true

# ── Step 4: Install system dependencies on master1 ───────────────
- name: Install git, pip, venv, rsync on master1
  apt:
    name:
      - git
      - python3-pip
      - python3-venv
      - python3-netaddr
      - rsync
    state: present
    update_cache: yes

# ── Step 5: Clone Kubespray ───────────────────────────────────────
- name: Remove existing Kubespray folder if present
  file:
    path: "/home/{{ ssh_user }}/kubespray"
    state: absent

- name: Clone Kubespray {{ kubespray_version }}
  git:
    repo: "{{ kubespray_repo }}"
    version: "{{ kubespray_version }}"
    dest: "/home/{{ ssh_user }}/kubespray"
    force: yes

- name: Set correct ownership on kubespray folder
  file:
    path: "/home/{{ ssh_user }}/kubespray"
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    recurse: yes

- name: Remove unneeded git/CI directories
  file:
    path: "/home/{{ ssh_user }}/kubespray/{{ item }}"
    state: absent
  loop:
    - .git
    - .github
    - .gitlab-ci.yml

# ── Step 6: pip install into venv ────────────────────────────────
- name: Create Python venv for Kubespray
  command: python3 -m venv /home/{{ ssh_user }}/kubespray/venv
  args:
    creates: "/home/{{ ssh_user }}/kubespray/venv/bin/activate"

- name: pip install -r requirements.txt inside venv
  shell: |
    source /home/{{ ssh_user }}/kubespray/venv/bin/activate
    pip install -r /home/{{ ssh_user }}/kubespray/requirements.txt
  args:
    executable: /bin/bash
  changed_when: true

- name: pip install extra packages into venv
  shell: |
    source /home/{{ ssh_user }}/kubespray/venv/bin/activate
    pip install netaddr jmespath ruamel.yaml
  args:
    executable: /bin/bash
  changed_when: false

# ── Step 7: Write inventory and config files ──────────────────────
- name: Copy node_info.json to master1
  copy:
    src: ../inventory/node_info.json
    dest: /tmp/node_info.json
  become: true

- name: Read node_info.json from master1
  slurp:
    src: /tmp/node_info.json
  register: _node_info_raw

- name: Set node_info fact
  set_fact:
    node_info: "{{ _node_info_raw.content | b64decode | from_json }}"
  when: node_info is not defined

- name: Write inventory/sample/inventory.ini with real node IPs
  template:
    src: inventory.ini.j2
    dest: "/home/{{ ssh_user }}/kubespray/inventory/sample/inventory.ini"
    owner: "{{ ssh_user }}"
    mode: '0644'

- name: Write addons.yml
  template:
    src: addons.yml.j2
    dest: "/home/{{ ssh_user }}/kubespray/inventory/sample/group_vars/k8s_cluster/addons.yml"
    owner: "{{ ssh_user }}"
    mode: '0644'

- name: Write k8s-cluster.yml
  template:
    src: k8s-cluster.yml.j2
    dest: "/home/{{ ssh_user }}/kubespray/inventory/sample/group_vars/k8s_cluster/k8s-cluster.yml"
    owner: "{{ ssh_user }}"
    mode: '0644'

# ── Step 8: Run cluster.yml ───────────────────────────────────────
# Launched via nohup so the process survives SSH disconnects.
# async:60/poll:0 fires and forgets cleanly.
# Polling loop reconnects every 30s to check /tmp/kubespray.pid.
# Pass/fail is determined by reading PLAY RECAP in /tmp/kubespray.log.
- name: Launch Kubespray cluster.yml in background on master1
  shell: |
    cd /home/{{ ssh_user }}/kubespray
    source venv/bin/activate
    nohup ansible-playbook -b -v \
      -i inventory/sample/inventory.ini \
      cluster.yml \
      > /tmp/kubespray.log 2>&1 &
    echo $! > /tmp/kubespray.pid
    echo "Kubespray launched with PID: $(cat /tmp/kubespray.pid)"
  args:
    executable: /bin/bash
  async: 60
  poll: 0

- name: Wait for Kubespray to complete (polls every 30s, max 60 min)
  shell: |
    pid=$(cat /tmp/kubespray.pid 2>/dev/null)
    if [ -z "$pid" ]; then
      echo "ERROR: PID file not found"
      exit 1
    fi
    if kill -0 "$pid" 2>/dev/null; then
      echo "RUNNING"
      exit 99
    fi
    if grep -q "PLAY RECAP" /tmp/kubespray.log; then
      if grep -E "failed=[1-9]|unreachable=[1-9]" /tmp/kubespray.log > /dev/null 2>&1; then
        echo "FAILED"
        exit 1
      fi
      echo "SUCCESS"
      exit 0
    else
      echo "FAILED - no PLAY RECAP found in log"
      exit 1
    fi
  args:
    executable: /bin/bash
  register: _kubespray_poll
  until: _kubespray_poll.rc != 99
  retries: 120    # 120 × 30s = 60 min max
  delay: 30
  failed_when: _kubespray_poll.rc == 1

- name: Show last 80 lines of Kubespray log
  shell: tail -80 /tmp/kubespray.log
  args:
    executable: /bin/bash
  register: _kubespray_tail
  changed_when: false

- name: Print Kubespray log tail
  debug:
    msg: "{{ _kubespray_tail.stdout_lines }}"

- name: Confirm Kubespray PLAY RECAP shows no failures
  shell: |
    grep "PLAY RECAP" /tmp/kubespray.log -A 20 | \
    grep -E "failed=[1-9]|unreachable=[1-9]"
  args:
    executable: /bin/bash
  register: _recap_check
  failed_when: _recap_check.rc == 0   # rc=0 means grep FOUND failures
  changed_when: false

# ── Step 9: Set up kubectl on master1 ────────────────────────────
- name: Verify admin.conf was created by Kubespray
  stat:
    path: /etc/kubernetes/admin.conf
  become: yes
  register: admin_conf

- name: Fail clearly if admin.conf is missing
  fail:
    msg: >
      /etc/kubernetes/admin.conf not found — Kubespray did not complete.
      Check: tail -100 /tmp/kubespray.log
  when: not admin_conf.stat.exists

- name: Create ~/.kube directory for {{ ssh_user }}
  file:
    path: "/home/{{ ssh_user }}/.kube"
    state: directory
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0700'

- name: Copy admin.conf to ~/.kube/config
  copy:
    src: /etc/kubernetes/admin.conf
    dest: "/home/{{ ssh_user }}/.kube/config"
    remote_src: yes
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0600'
  become: yes

- name: Verify cluster is healthy
  command: kubectl get nodes -o wide
  environment:
    KUBECONFIG: "/home/{{ ssh_user }}/.kube/config"
  register: kubectl_nodes
  retries: 5
  delay: 15
  until: kubectl_nodes.rc == 0

- name: Show cluster node status
  debug:
    msg: |
      ✅ Cluster is up!

      {{ kubectl_nodes.stdout }}