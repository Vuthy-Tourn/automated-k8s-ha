---
# ================================================================
# KUBESPRAY DEPLOY — runs on master1 only
#
# Steps:
#   1. Authorize SSH public key on ALL nodes (so master1 can reach them)
#   2. Push SSH private key to master1
#   3. Scan host keys on master1
#   4. Install system dependencies on master1
#   5. Clone Kubespray
#   6. pip install requirements into venv
#   7. Write inventory, addons.yml, k8s-cluster.yml, Justfile
#   8. Run cluster.yml
#   9. Set up kubectl
# ================================================================

# ── Step 1: Authorize public key on ALL nodes ─────────────────────
# master1 SSHes to all other nodes during cluster.yml — every node
# must trust the cluster key, not just master1
- name: Authorize cluster SSH public key on all nodes
  authorized_key:
    user: "{{ ssh_user }}"
    key: "{{ lookup('file', ssh_public_key) }}"
    state: present
  delegate_to: "{{ item }}"
  loop: "{{ groups['all'] }}"
  loop_control:
    label: "{{ item }}"

# ── Step 2: Push SSH key to master1 ──────────────────────────────
# master1 needs the private key to initiate SSH connections to other nodes
- name: Ensure .ssh directory exists on master1
  file:
    path: "/home/{{ ssh_user }}/.ssh"
    state: directory
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0700'

- name: Push cluster SSH private key to master1
  copy:
    src: "{{ ssh_private_key }}"
    dest: "/home/{{ ssh_user }}/.ssh/id_rsa"
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0600'

- name: Push cluster SSH public key to master1
  copy:
    src: "{{ ssh_public_key }}"
    dest: "/home/{{ ssh_user }}/.ssh/id_rsa.pub"
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0644'

# ── Step 3: Scan host keys on master1 ────────────────────────────
# Prevents "host key verification failed" when master1 SSHes to other nodes
- name: Scan and register all node host keys on master1
  shell: |
    ssh-keyscan -t rsa {{ hostvars[item]['ip'] }} \
      >> /home/{{ ssh_user }}/.ssh/known_hosts 2>/dev/null
    sort -u /home/{{ ssh_user }}/.ssh/known_hosts \
      -o /home/{{ ssh_user }}/.ssh/known_hosts
  args:
    executable: /bin/bash
  loop: "{{ groups['all'] }}"
  changed_when: true

# ── Step 4: Install system dependencies on master1 ───────────────
- name: Install git, pip, venv, rsync on master1
  apt:
    name:
      - git
      - python3-pip
      - python3-venv
      - python3-netaddr
      - rsync
    state: present
    update_cache: yes

# - name: Install 'just' task runner if not present
#   shell: |
#     if ! command -v just >/dev/null 2>&1; then
#       VERSION=$(curl -s https://api.github.com/repos/casey/just/releases/latest \
#         | grep tag_name | cut -d '"' -f4)
#       curl -L "https://github.com/casey/just/releases/download/${VERSION}/just-${VERSION}-x86_64-unknown-linux-musl.tar.gz" \
#         -o /tmp/just.tar.gz
#       tar -xzf /tmp/just.tar.gz -C /usr/local/bin just
#       chmod +x /usr/local/bin/just
#     fi
#   args:
#     executable: /bin/bash
#   changed_when: false

# ── Step 5: Clone Kubespray ───────────────────────────────────────
- name: Remove existing Kubespray folder if present
  file:
    path: "/home/{{ ssh_user }}/kubespray"
    state: absent

- name: Clone Kubespray {{ kubespray_version }}
  git:
    repo: "{{ kubespray_repo }}"
    version: "{{ kubespray_version }}"
    dest: "/home/{{ ssh_user }}/kubespray"
    force: yes

- name: Set correct ownership on kubespray folder
  file:
    path: "/home/{{ ssh_user }}/kubespray"
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    recurse: yes

- name: Remove unneeded git/CI directories
  file:
    path: "/home/{{ ssh_user }}/kubespray/{{ item }}"
    state: absent
  loop:
    - .git
    - .github
    - .gitlab-ci.yml

# ── Step 6: pip install into venv ────────────────────────────────
- name: Create Python venv for Kubespray
  command: python3 -m venv /home/{{ ssh_user }}/kubespray/venv
  args:
    creates: "/home/{{ ssh_user }}/kubespray/venv/bin/activate"

- name: pip install -r requirements.txt inside venv
  shell: |
    source /home/{{ ssh_user }}/kubespray/venv/bin/activate
    pip install -r /home/{{ ssh_user }}/kubespray/requirements.txt
  args:
    executable: /bin/bash
  changed_when: true

- name: pip install extra packages into venv
  shell: |
    source /home/{{ ssh_user }}/kubespray/venv/bin/activate
    pip install netaddr jmespath ruamel.yaml
  args:
    executable: /bin/bash
  changed_when: false

# ── Step 7: Write inventory and config files ──────────────────────
- name: Copy node_info.json to remote host
  copy:
    src: ../inventory/node_info.json
    dest: /tmp/node_info.json
  become: true

- name: Load node_info from file on remote host
  set_fact:
    node_info: "{{ lookup('file', '/tmp/node_info.json') | from_json }}"
  when: node_info is not defined

- name: Write inventory/sample/inventory.ini with real node IPs
  template:
    src: inventory.ini.j2
    dest: "/home/{{ ssh_user }}/kubespray/inventory/sample/inventory.ini"
    owner: "{{ ssh_user }}"
    mode: '0644'

- name: Write addons.yml
  template:
    src: addons.yml.j2
    dest: "/home/{{ ssh_user }}/kubespray/inventory/sample/group_vars/k8s_cluster/addons.yml"
    owner: "{{ ssh_user }}"
    mode: '0644'

- name: Write k8s-cluster.yml
  template:
    src: k8s-cluster.yml.j2
    dest: "/home/{{ ssh_user }}/kubespray/inventory/sample/group_vars/k8s_cluster/k8s-cluster.yml"
    owner: "{{ ssh_user }}"
    mode: '0644'

# - name: Write Justfile
#   template:
#     src: Justfile.j2
#     dest: "/home/{{ ssh_user }}/kubespray/Justfile"
#     owner: "{{ ssh_user }}"
#     mode: '0755'

# ── Step 8: Run cluster.yml ───────────────────────────────────────
# Runs entirely on master1 — it SSHes to all other nodes internally
# exit ${PIPESTATUS[0]} captures ansible-playbook exit code through tee pipe
- name: Run Kubespray cluster.yml from master1 (35-50 min)
  shell: |
    cd /home/{{ ssh_user }}/kubespray
    source venv/bin/activate
    ansible-playbook -b -v -i inventory/sample/inventory.ini \
    cluster.yml
    exit ${PIPESTATUS[0]}
  args:
    executable: /bin/bash
  timeout: 3600
  register: kubespray_run

- name: Confirm Kubespray succeeded
  fail:
    msg: |
      Kubespray failed — SSH to master1 and check:
        tail -100 ~/kubespray-install.log
  when: kubespray_run.failed is defined and kubespray_run.failed

# ── Step 9: Set up kubectl on master1 ────────────────────────────
- name: Verify admin.conf was created by Kubespray
  stat:
    path: /etc/kubernetes/admin.conf
  register: admin_conf

- name: Fail clearly if admin.conf is missing
  fail:
    msg: >
      /etc/kubernetes/admin.conf not found — Kubespray did not complete.
      Check: tail -100 ~/kubespray-install.log
  when: not admin_conf.stat.exists

- name: Create ~/.kube directory for {{ ssh_user }}
  file:
    path: "/home/{{ ssh_user }}/.kube"
    state: directory
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0700'

- name: Copy admin.conf to ~/.kube/config
  copy:
    src: /etc/kubernetes/admin.conf
    dest: "/home/{{ ssh_user }}/.kube/config"
    remote_src: yes
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
    mode: '0600'

- name: Verify cluster is healthy
  command: kubectl get nodes -o wide
  environment:
    KUBECONFIG: "/home/{{ ssh_user }}/.kube/config"
  register: kubectl_nodes
  retries: 5
  delay: 15
  until: kubectl_nodes.rc == 0

- name: Show cluster node status
  debug:
    msg: |
      ✅ Cluster is up!

      {{ kubectl_nodes.stdout }}